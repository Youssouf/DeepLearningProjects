{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filip Jurić <fijur20@student.sdu.dk>\n",
    "\n",
    "Lucas Olai Jarlkov Olsen <luols17@student.sdu.dk>\n",
    "\n",
    "Martin Kristian Lorenzen <marlo17@student.sdu.dk>\n",
    "\n",
    "Sebastian Eklund Larsen <selar16@student.sdu.dk>\n",
    "\n",
    "Youssouf Souare <yosou20@student.sdu.dk>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Data prep\n",
    "\n",
    "Download and extract dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATASET DOWNLOAD\n",
    "\"\"\"\n",
    "import os.path\n",
    "\n",
    "DATASET = \"pneumonia_project_2020_2.zip\"\n",
    "URL = \"https://nextcloud.sdu.dk/index.php/s/s7bA69T4Stog6wo/download\"\n",
    "\n",
    "# Download if it does not exist\n",
    "if not os.path.isfile(DATASET):\n",
    "    import requests\n",
    "    print(\"Downloading file...\")\n",
    "    downfile = requests.get(URL)\n",
    "    open(DATASET, 'wb').write(downfile.content)\n",
    "    del downfile\n",
    "else:\n",
    "    print(\"Skipping download...\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OVERLY COMPLICATED DATASET EXTRACTION\n",
    "\"\"\"\n",
    "EXTRACT = False # Manual toggle for when rerunning\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DATA_FOLDERS = [\"training\", \"evaluation\"]\n",
    "\n",
    "if EXTRACT:\n",
    "    import os\n",
    "    import shutil\n",
    "    from zipfile import ZipFile\n",
    "\n",
    "    print(\"Extracing dataset...\")\n",
    "\n",
    "    # Remove any previous extractions\n",
    "    for dir in DATA_FOLDERS:\n",
    "        try:\n",
    "            rmtree(dir)\n",
    "        except: pass\n",
    "\n",
    "    with ZipFile(DATASET, 'r') as zip:\n",
    "        for info in zip.infolist():\n",
    "            if info.filename[-1] == '/': # Skip directories\n",
    "                continue\n",
    "            if info.filename.endswith(\".pdf\"): # Skip PDFs\n",
    "                continue \n",
    "            # Identify if Filtered or Encoded\n",
    "            folder = \"other\"\n",
    "            if info.filename.startswith(\"project_2020/filtered/\"):\n",
    "                folder = \"training\"\n",
    "            elif info.filename.startswith(\"project_2020/encoded\"):\n",
    "                folder = \"evaluation\"\n",
    "            # Read label (PNEUMONIA/NORMAL)\n",
    "            label = info.filename.split('/')[-2]\n",
    "            # Extraction magic\n",
    "            info.filename = os.path.basename(info.filename)\n",
    "            zip.extract(info, f\"{folder}/{label}\")\n",
    "else: \n",
    "    print(\"Skiping dataset extraction...\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Layers\n",
    "\n",
    "To create your model, you will need different layers. You are forbidden fromusing the built in dense, convolution, and pooling layers. Instead, you mustimplement these 3 layers on your own. You can use the code you have developed for class. A good place to start off is always the Keras documentation. [You can view the documentation on custom layers by clicking HERE](https://keras.io/layers/writing-your-own-keras-layers/).\n",
    "\n",
    "You must present three classes, Dense, Convolution2D, and Pooling.\n",
    "\n",
    "You can add additional parameters to these layers as you might need them,such as stride, kernel size, different types of pooling such as average pooling,max pooling, etc.\n",
    "\n",
    "You are allowed to use all backend functions like `K.dot(x, y)` and `K.conv2d(x, kernel)` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Layer Imports\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dense Layer Implementation\n",
    "\"\"\"\n",
    "\n",
    "#might want to change name to something unique\n",
    "class CustomDense(Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        super(CustomDense, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name=\"weights\",shape=(input_shape[1], self.units),\n",
    "                                 initializer=\"uniform\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"bias\",shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        super(CustomDense, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return(input_shape[0], self.units)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.activation(K.dot(x, self.w) + self.b)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(CustomDense, self).get_config()\n",
    "        config.update({\"units\": self.units, \"activation\":self.activation})\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolution Layer Implementation Goes Here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Convolution2D(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=(1, 1), activation=None, padding='valid', **kwargs):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.padding = padding\n",
    "        self.input_spec = keras.layers.InputSpec(ndim=4)\n",
    "        super(Convolution2D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        channel_amount = int(input_shape[-1])\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(*self.kernel_size, channel_amount, self.filters),\n",
    "                                      initializer='glorot_uniform')\n",
    "        super(Convolution2D, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Do the convolution calculation\n",
    "        return self.activation(K.conv2d(inputs, self.kernel, self.strides, self.padding))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Determine how to compute the output shape\n",
    "        return *input_shape[:-1], self.filters\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Convolution2D, self).get_config()\n",
    "\n",
    "        return {\n",
    "            **config, **{\n",
    "                \"filters\": self.filters,\n",
    "                \"kernel_size\": self.kernel_size,\n",
    "                \"strides\": self.strides,\n",
    "                \"activation\": self.activation,\n",
    "                \"padding\": self.padding,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        layer = cls(**config)\n",
    "        layer.kernel = config['kernel']\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Pooling Layer Implementation\n",
    "\"\"\"\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "\n",
    "class Pooling2d(Layer):\n",
    "    def __init__(self, pool_size=(2,2), strides=None, padding=\"valid\", pool_mode=\"max\", **kwargs):\n",
    "        self.pool_size = pool_size\n",
    "        self.padding = padding\n",
    "        self.pool_mode = pool_mode\n",
    "        if strides == None:\n",
    "            self.strides = pool_size\n",
    "        else:\n",
    "            self.strides = strides\n",
    "        \n",
    "        super(Pooling2d, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(Pooling2d, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape): #assumes channels last\n",
    "        col = int(input_shape[2])\n",
    "        row = int(input_shape[1])\n",
    "        if self.padding == \"same\":\n",
    "            out_rows = int(math.floor((row - 1) / self.strides[0]) + 1)\n",
    "            out_cols = int(math.floor((col - 1) / self.strides[1]) + 1)\n",
    "            return tf.TensorShape( (input_shape[0], out_rows, out_cols, input_shape[3]))\n",
    "        \n",
    "        else: # padding == valid\n",
    "            out_rows = int(math.floor((row - self.pool_size[0]) / self.strides[0]) + 1)\n",
    "            out_cols = int(math.floor((col - self.pool_size[1]) / self.strides[1]) + 1)\n",
    "            \n",
    "            return tf.TensorShape((input_shape[0], out_rows, out_cols, input_shape[3]))\n",
    "            \n",
    "    def call(self, inp):\n",
    "        output = K.pool2d(inp,self.pool_size, self.strides, self.padding, pool_mode = self.pool_mode)\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(CustomLayer, self).get_config()\n",
    "        config.update({\"pool_size\": self.poolsize,\"strides\" : self.strides, \"padding\": self.padding, \"pool_mode\": self.pool_mode})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Model\n",
    "\n",
    "You must now build your model.  To do this, use your own layers, as well as theother layers from the Keras library except  the  three forbidden layers (Dense,Convolution2D, pooling).  This means you can use for example Flatten from theKeras library.\n",
    "\n",
    "You must now build your model. To do this, use your own layers, as well as theother layers from the Keras library except the three forbidden layers (Dense,Convolution2D, pooling). This means you can use for example Flatten from theKeras library.\n",
    "\n",
    "Try to make your model as good as possible. A quick list of things you couldtry:\n",
    "* Adding different kinds of regularization\n",
    "* Adding different kinds of data augmentation\n",
    "* Varying your model parameters such as kernel size, amount of units, acti-vation functions\n",
    "* The structure of your model. Make it deeper and slimmer, make it moreshallow but wider\n",
    "\n",
    "Especially the last suggestion can give large gains. Here is a couple papersyou can read, if you want to know the state-of-the-art networks:\n",
    "\n",
    "* [InceptionV33](https://arxiv.org/abs/1512.00567)\n",
    "* [DenseNet](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "It is important that you resize your images to 224x224, in order for yournetwork to work for task 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build Datagenerator\n",
    "\"\"\"\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_CHANNELS = 1\n",
    "IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "INPUT_SHAPE = (IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"training/\",\n",
    "    label_mode=\"binary\",\n",
    "    color_mode=\"grayscale\",\n",
    "    image_size=IMG_SIZE,\n",
    "    seed=RANDOM_SEED,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"training/\",\n",
    "    label_mode=\"binary\",\n",
    "    color_mode=\"grayscale\",\n",
    "    image_size=IMG_SIZE,\n",
    "    seed=RANDOM_SEED,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build & Compile Model\n",
    "\"\"\"\n",
    "\n",
    "INITIAL_EPOCH=0\n",
    "\n",
    "# TODO (2) Switch to our custom layers (Dense, Convolution & Pooling)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255.0, input_shape=INPUT_SHAPE),\n",
    "\n",
    "    Convolution2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "        \n",
    "    Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    \n",
    "    Convolution2D(filters=128, kernel_size=(3, 3), activation='relu'), # bigger\n",
    "    tf.keras.layers.BatchNormalization(), \n",
    "    tf.keras.layers.SpatialDropout2D(0.2), \n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5), \n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train Model\n",
    "\"\"\"\n",
    "EPOCHS = INITIAL_EPOCH + 1\n",
    "\n",
    "now = \"run\"\n",
    "\n",
    "callbacks = [\n",
    "# Commented out to save memory (figure out save freq)\n",
    "     tf.keras.callbacks.ModelCheckpoint(\n",
    "         \"checkpoints/\" + now  +\"{epoch}.h5\",\n",
    "         monitor='val_accuracy'),\n",
    "#         mode='max',\n",
    "#         save_best_only=True),\n",
    "#    tf.keras.callbacks.TensorBoard('logs/', histogram_freq=1)\n",
    " ]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=INITIAL_EPOCH,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_dataset,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "INITIAL_EPOCH = EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at data & pick how many epochs (tensorboard is better)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.3))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(history.history['loss'], label='Training loss')\n",
    "ax.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(history.history['accuracy'], label='Training accuracy')\n",
    "ax.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Evaluation\n",
    "\n",
    "For this last task, you’re given a new dataset that you must evaluate your model on. The new dataset is different from the one you’ve been training on. You will find it in the `evaluation` directory.\n",
    "\n",
    "The new dataset consists of 30 normal cases, and 21 pneumonia cases. This time, they’re not in a jpeg format, but are txt files. Each file contains a 224x224 matrix of the pixel values of an image. As the matrix is 2D, we only have 1 color channel. If your model was trained on 3 color channels, you must convertthis monochrome 1-channel data to a 3-channel RGB similar to your training data.\n",
    "\n",
    "To do this, you are required to write a custom Keras generator class. Onceagain, I recommend you check the docs. [You can see the Sequence base class HERE](https://keras.io/utils/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Evaluation Data Generator\n",
    "\"\"\"\n",
    "import tensorflow as ts\n",
    "import math\n",
    "\n",
    "class Generator(ts.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size=32):\n",
    "        pneumonia = os.listdir(\"evaluation/PNEUMONIA/\")\n",
    "        normal = os.listdir(\"evaluation/NORMAL/\")\n",
    "\n",
    "        self.file_list = []\n",
    "        self.class_list = []\n",
    "\n",
    "        for f in pneumonia:\n",
    "            self.file_list.append(\"evaluation/PNEUMONIA/\" + f)\n",
    "            self.class_list.append(1)\n",
    "        for f in normal:\n",
    "            self.file_list.append(\"evaluation/NORMAL/\" + f)\n",
    "            self.class_list.append(0)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.file_list) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.file_list[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.class_list[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array([np.loadtxt(filepath) for filepath in batch_x]), np.array(batch_y)\n",
    "    \n",
    "evaluation_dataset = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate model\n",
    "\"\"\"\n",
    "model.evaluate(evaluation_dataset)\n",
    "model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix and Classification  Report \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score,\\\n",
    "cohen_kappa_score, accuracy_score, classification_report\n",
    "\n",
    "y_pred = model.predict(evaluation_dataset)\n",
    "y_pred_r = np.round(y_pred)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(evaluation_dataset.class_list, y_pred_r))\n",
    "                    \n",
    "\n",
    "print('classification Report')\n",
    "target_name = ['NORMAL', 'PNEUMONIA']\n",
    "print(classification_report(evaluation_dataset.class_list, y_pred_r, target_names = target_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
